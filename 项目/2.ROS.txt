小车采用远程SSH 连接方式控制

框架：摄像头（识别锥桶）、雷达（和图像融合获取锥桶位置）、CPU+MCU

问题：
在官方的示例里最初采取的方法是：
#RBG转HSV、二值化、边缘检测、寻找边界，返回检测矩形框的像素坐标
	消防栓的红色门导致路径规划有误
	解决方法：增加形状判断的部分，如果高>宽+上界x范围小于下界才算桶

项目中使用了哪些ROS核心功能？如导航（move_base）、路径规划（global_planner）或传感器整合（sensor fusion）

在ROS中，如何实现多模块间的通信？例如，目标检测模块与路径规划模块之间的数据交互
#话题、节点、发布者、订阅者

是否在项目中使用了SLAM（同时定位与建图）技术？选择的是哪种算法（如GMapping、Cartographer、ORB-SLAM）？

如何处理地图构建中的噪声问题，或者传感器的漂移问题？

在ROS开发中，是否使用了Gazebo或RViz进行仿真？仿真测试与实际无人车测试结果是否有差异？如何处理这些差异？

HSV颜色空间的亮度调节是如何实现的？为什么选择使用HSV而不是RGB？
#色相+饱和度+亮度；HSV不改变其他通道

针对不同光照条件下的鲁棒性问题，如何验证优化的效果？是否进行了不同环境下的数据采集和测试？

提到使用了Canny边缘检测，边缘检测的结果如何与路径规划结合？是否存在误检或漏检问题？

Canny的参数（如阈值）是如何选择的？是否进行了自动调优？
#两个阈值、手动

在目标检测中，是否考虑过其他方法（如基于深度学习的YOLO或SSD）？如果没有使用，主要原因是什么？

无人车如何实现精准停靠？是基于视觉检测标志点还是通过里程计和GPS？

如果比赛中存在导航偏差（如目标点停不准）是如何纠正的？

地图是预先生成的还是实时构建的？地图数据的精度对导航有什么影响？

在复杂环境（如障碍物密集、光线不足）下，系统性能是否受到影响？如何解决？

OpenCV中是否使用了其他方法辅助导航（如形状检测、模板匹配）？

在视觉目标识别中，是否遇到过算法实时性的问题？是如何优化的？

项目中有哪些性能指标（如识别准确率、导航精度）？如何对这些指标进行测试和评估的？

比赛中，识别错误率或导航失败率有多少？如何通过调优算法或参数降低失败率？

无人车的计算资源是否满足所有任务的需求？是否进行过资源的优化调度？

项目中使用了哪些传感器（如激光雷达、超声波、摄像头）？它们在自主导航中的具体作用是什么？

如果多个传感器数据出现冲突（如激光雷达和摄像头的目标检测结果不一致）如何处理？

ROS在硬件调试过程中是否遇到过驱动问题（如设备连接、数据采集异常）？是如何解决的？

无人车是否使用了IMU进行姿态校正？IMU数据与导航数据的融合如何实现？

在项目中，您具体负责哪些模块？您与团队成员的协作方式是怎样的？

如果团队中某些模块（如感知或规划）进展缓慢，您是如何协调推进的？

比赛中是否有固定的环境模拟，还是存在随机因素？如何应对随机环境中的突发问题？

您认为比赛中您的团队表现的优势和不足分别是什么？

比赛中的最大挑战是什么？是视觉识别的鲁棒性、路径规划的实时性，还是无人车的整体稳定性？您具体是如何应对的？

如果未来要扩展项目，例如引入多传感器融合或深度学习，您认为系统需要做哪些调整？

如果比赛要求无人车执行更多任务（如动态目标跟踪或多目标识别），您会如何设计和优化现有系统？

您认为这个项目成果能否在实际场景中应用（如无人配送车或自动驾驶）？如果可以，您认为需要哪些额外的改进？